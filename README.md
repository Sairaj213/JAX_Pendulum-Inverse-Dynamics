# ğŸ•°ï¸ JAX_Pendulum-Inverse-Dynamics

The Project uses physics-based simulation and optimization implemented using JAX, it demonstrates how to recover the initial conditions of a pendulum system using gradient-based optimization from JAX. A simulated target trajectory is first generated using known initial values for the pendulumâ€™s angle and angular velocity.using only this trajectory, the project attempts to learn the original initial conditions by minimizing the mean squared error between a predicted trajectory (from guessed values) and the target one. The differential equations are solved using odeint, gradients are computed using jax.grad, and optimization is done via simple gradient descent.

<br>
<br>

# ğŸ—‚ï¸ Project Structure
```markdown-tree
ğŸ“ JAX_Pendulum-Inverse-Dynamics
â”œâ”€â”€ main.py                         # Entry point
â”œâ”€â”€ setup_imports.py                # Handles JAX imports, precision config, and random seed
â”œâ”€â”€ pendulum_dynamics.py            # Defines pendulum ODE and physical constants
â”œâ”€â”€ simulate.py                     # JIT-compiled function to run pendulum simulation
â”œâ”€â”€ target_trajectory.py            # Generates and stores the target trajectory
â”œâ”€â”€ loss.py                         # Mean squared error computation
â”œâ”€â”€ config.yaml                     # Centralized configuration for simulation and optimization
â”œâ”€â”€ config_loader.py                # Loads and parses config.yaml
â”œâ”€â”€ objective.py                    # Objective function and gradient computation
â”œâ”€â”€ optimize.py                     # Gradient descent optimization loop
â”œâ”€â”€ visualize_progress.py           # Loss + parameter history visualization
â”œâ”€â”€ compare_trajectory.py           # Final comparison plot between target and optimized output
â”œâ”€â”€ requirements.txt                # Dependencies list
â””â”€â”€ README.md                       # Project documentation
```

<br>
<br>

# ğŸš€ Getting Started 

### ğŸ“¥ Clone the Repository
First, clone this repository to your local machine:
```bash
git clone https://github.com/Sairaj213/JAX_Pendulum-Inverse-Dynamics.git

cd JAX_Pendulum-Inverse-Dynamics
```
### âš™ï¸ Install Requirements

Ensure youâ€™re using Python 3.9+. Then, install all necessary dependencies:

```bash
pip install -r requirements.txt
```

For GPU acceleration, refer to [JAX's official installation guide](https://github.com/jax-ml/jax#installation) based on your CUDA version.

### â–¶ï¸ Run the Simulation

Execute the main script:

```bash
python main.py
```
Youâ€™ll see:

* Basic setup logs (JAX version, constants)

* Target pendulum simulation

* Optimization progress

* Visualization of convergence

* Final comparison of optimized vs. true trajectory

<br>
<br>

# âš™ï¸ Customization / Parameters

#### This project offers several tunable parameters to adapt the simulation and optimization behavior to your needs.
You can check out in file [``config.yaml``](./config.yaml)
<br>

### ğŸ”§ Simulation Parameters

| Parameter   | Description                      | Default Value |
| ----------- | -------------------------------- | ------------- |
| `g`         | Acceleration due to gravity      | `9.81`        |
| `L`         | Length of the pendulum           | `1.0`         |
| `T`         | Total simulation time in seconds | `10.0`        |
| `num_steps` | Number of time steps to simulate | `500`         |
| `total_time` | Total duration of the simulation (in seconds).  | `10.0`         |

<br>

### ğŸ¯ Target Trajectory

| Parameter       | Description                                            | Default Value |
| --------------- | ------------------------------------------------------ | ------------- |
| `initial_theta` | Initial angle (in radians) for target trajectory       | `0.7854`      |
| `initial_omega` | Initial angular velocity (rad/s) for target trajectory | `0.0`         |

<br>

### ğŸ§  Optimization Parameters

| Parameter       | Description                            | Default Value |
| --------------- | -------------------------------------- | ------------- |
| `learning_rate` | Step size for gradient descent         | `0.05`        |
| `epochs`        | Number of optimization iterations      | `1000`        |
| `initial_guess` | Randomized initial guess for \[Î¸â‚€, Ï‰â‚€] | Numpy random  |
| `seed` | Random seed for reproducibility of initial guess. | `42`  |

